{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54acdd7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:16.009590Z",
     "iopub.status.busy": "2023-11-01T09:38:16.009263Z",
     "iopub.status.idle": "2023-11-01T09:38:19.358425Z",
     "shell.execute_reply": "2023-11-01T09:38:19.357373Z"
    },
    "papermill": {
     "duration": 3.368303,
     "end_time": "2023-11-01T09:38:19.360731",
     "exception": false,
     "start_time": "2023-11-01T09:38:15.992428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.0\n",
      "torchvision version: 0.15.1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "except:\n",
    "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
    "    !pip3 install -U torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(f\"torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0395db62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.392511Z",
     "iopub.status.busy": "2023-11-01T09:38:19.392075Z",
     "iopub.status.idle": "2023-11-01T09:38:19.492552Z",
     "shell.execute_reply": "2023-11-01T09:38:19.491606Z"
    },
    "papermill": {
     "duration": 0.119266,
     "end_time": "2023-11-01T09:38:19.494975",
     "exception": false,
     "start_time": "2023-11-01T09:38:19.375709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Following cell contains code to train and test model, as it is general function, can be reused for almost any model of similar task\n",
    "'''\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device) -> Tuple[float, float]:\n",
    "    '''\n",
    "    Trains model for single epoch.\n",
    "\n",
    "    Args:\n",
    "    model: Model to be trained\n",
    "    dataloader: Dataloader instance\n",
    "    loss_fn: Loss function to calculate loss\n",
    "    optimizer: Optimizer to minimize loss\n",
    "    device: target device\n",
    "\n",
    "    Returns:\n",
    "    Tuple of (train_loss, train_accuracy)\n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0.00, 0.00\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc+=(y_pred_class==y).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss = train_loss/len(dataloader)\n",
    "    train_acc = train_acc/len(dataloader)\n",
    "\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for a single epoch.\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "    Args:\n",
    "    model: A PyTorch model to be tested.\n",
    "    dataloader: A DataLoader instance for the model to be tested on.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "    Returns:\n",
    "    A tuple of testing loss and testing accuracy metrics.\n",
    "    In the form (test_loss, test_accuracy). For example:\n",
    "    (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0.00, 0.00\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device)->Dict[str, int]:\n",
    "    \"\"\"Trains and tests a PyTorch model.\n",
    "    Passes a target PyTorch models through train_step() and test_step()\n",
    "    functions for a number of epochs, training and testing the model\n",
    "    in the same epoch loop.\n",
    "    Calculates, prints and stores evaluation metrics throughout.\n",
    "    Args:\n",
    "    model: A PyTorch model to be trained and tested.\n",
    "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
    "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
    "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
    "    epochs: An integer indicating how many epochs to train for.\n",
    "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "    Returns:\n",
    "    A dictionary of training and testing loss as well as training and\n",
    "    testing accuracy metrics. Each metric has a value in a list for \n",
    "    each epoch.\n",
    "    In the form: {train_loss: [...],\n",
    "              train_acc: [...],\n",
    "              test_loss: [...],\n",
    "              test_acc: [...]} \n",
    "    For example if training for epochs=2: \n",
    "             {train_loss: [2.0616, 1.0537],\n",
    "              train_acc: [0.3945, 0.3945],\n",
    "              test_loss: [1.2641, 1.5706],\n",
    "              test_acc: [0.3400, 0.2973]} \n",
    "    \"\"\"\n",
    "    results = {\"train_loss\": [],\n",
    "               \"train_acc\": [],\n",
    "               \"test_loss\": [],\n",
    "               \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                dataloader=train_dataloader,\n",
    "                                loss_fn=loss_fn,\n",
    "                                optimizer=optimizer,\n",
    "                                device=device)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "          dataloader=test_dataloader,\n",
    "          loss_fn=loss_fn,\n",
    "          device=device)\n",
    "        \n",
    "        print(\n",
    "          f\"Epoch: {epoch+1} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | \"\n",
    "          f\"train_acc: {train_acc:.4f} | \"\n",
    "          f\"test_loss: {test_loss:.4f} | \"\n",
    "          f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0362def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.526767Z",
     "iopub.status.busy": "2023-11-01T09:38:19.525975Z",
     "iopub.status.idle": "2023-11-01T09:38:19.534437Z",
     "shell.execute_reply": "2023-11-01T09:38:19.533585Z"
    },
    "papermill": {
     "duration": 0.026324,
     "end_time": "2023-11-01T09:38:19.536330",
     "exception": false,
     "start_time": "2023-11-01T09:38:19.510006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This cell contains functionality to create custom dataloader\n",
    "'''\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloader(\n",
    "        train_dir: str,\n",
    "        test_dir: str,\n",
    "        transform: transforms.Compose,\n",
    "        batch_size: int,\n",
    "        num_workers: int=NUM_WORKERS\n",
    "    ):\n",
    "    \"\"\"Creates training and testing DataLoaders.\n",
    "        Takes in a training directory and testing directory path and turns\n",
    "        them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "        Args:\n",
    "            train_dir: Path to training directory.\n",
    "            test_dir: Path to testing directory.\n",
    "            transform: torchvision transforms to perform on training and testing data.\n",
    "            batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "            num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "        Returns:\n",
    "            A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "            Where class_names is a list of the target classes.\n",
    "            Example usage:\n",
    "            train_dataloader, test_dataloader, class_names = \\\n",
    "                = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                                    test_dir=path/to/test_dir,\n",
    "                                    transform=some_transform,\n",
    "                                    batch_size=32,\n",
    "                                    num_workers=4)\n",
    "    \"\"\"\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=False,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a6fe2c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.568147Z",
     "iopub.status.busy": "2023-11-01T09:38:19.567490Z",
     "iopub.status.idle": "2023-11-01T09:38:19.572121Z",
     "shell.execute_reply": "2023-11-01T09:38:19.571256Z"
    },
    "papermill": {
     "duration": 0.022616,
     "end_time": "2023-11-01T09:38:19.574005",
     "exception": false,
     "start_time": "2023-11-01T09:38:19.551389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d65c337e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.606602Z",
     "iopub.status.busy": "2023-11-01T09:38:19.606269Z",
     "iopub.status.idle": "2023-11-01T09:38:19.613653Z",
     "shell.execute_reply": "2023-11-01T09:38:19.612825Z"
    },
    "papermill": {
     "duration": 0.02633,
     "end_time": "2023-11-01T09:38:19.615517",
     "exception": false,
     "start_time": "2023-11-01T09:38:19.589187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(results):\n",
    "    loss = results[\"train_loss\"]\n",
    "    test_loss = results[\"test_loss\"]\n",
    "\n",
    "    accuracy = results[\"train_acc\"]\n",
    "    test_accuracy = results[\"test_acc\"]\n",
    "\n",
    "    epochs = range(len(results[\"train_loss\"]))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label=\"train_loss\")\n",
    "    plt.plot(epochs, test_loss, label=\"test_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n",
    "    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "352257b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.647002Z",
     "iopub.status.busy": "2023-11-01T09:38:19.646679Z",
     "iopub.status.idle": "2023-11-01T09:38:19.699531Z",
     "shell.execute_reply": "2023-11-01T09:38:19.698543Z"
    },
    "papermill": {
     "duration": 0.070862,
     "end_time": "2023-11-01T09:38:19.701458",
     "exception": false,
     "start_time": "2023-11-01T09:38:19.630596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3ddd133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.733514Z",
     "iopub.status.busy": "2023-11-01T09:38:19.732713Z",
     "iopub.status.idle": "2023-11-01T09:38:19.739199Z",
     "shell.execute_reply": "2023-11-01T09:38:19.738104Z"
    },
    "papermill": {
     "duration": 0.025341,
     "end_time": "2023-11-01T09:38:19.741938",
     "exception": false,
     "start_time": "2023-11-01T09:38:19.716597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/resizedImg/images/train',\n",
       " '/kaggle/working/resizedImg/images/test')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '/kaggle/working/resizedImg/images'\n",
    "\n",
    "train_dir = image_path+\"/train\"\n",
    "test_dir = image_path+\"/test\"\n",
    "\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d589cc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.773724Z",
     "iopub.status.busy": "2023-11-01T09:38:19.773380Z",
     "iopub.status.idle": "2023-11-01T09:38:19.778385Z",
     "shell.execute_reply": "2023-11-01T09:38:19.777406Z"
    },
    "papermill": {
     "duration": 0.023629,
     "end_time": "2023-11-01T09:38:19.780668",
     "exception": false,
     "start_time": "2023-11-01T09:38:19.757039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually created transform: Compose(\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "manual_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "print(f\"Manually created transform: {manual_transforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc03c3fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:38:19.813008Z",
     "iopub.status.busy": "2023-11-01T09:38:19.812195Z",
     "iopub.status.idle": "2023-11-01T09:38:20.360152Z",
     "shell.execute_reply": "2023-11-01T09:38:20.358682Z"
    },
    "papermill": {
     "duration": 0.565847,
     "end_time": "2023-11-01T09:38:20.361863",
     "exception": true,
     "start_time": "2023-11-01T09:38:19.796016",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/working/resizedImg/images/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;66;03m# this is lower than the ViT paper but it's because we're starting small\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create data loaders\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m train_dataloader, test_dataloader, class_names \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanual_transforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# use manually created transforms\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m train_dataloader, test_dataloader, class_names\n",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[0;34m(train_dir, test_dir, transform, batch_size, num_workers)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataloader\u001b[39m(\n\u001b[1;32m     12\u001b[0m         train_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     13\u001b[0m         test_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         num_workers: \u001b[38;5;28mint\u001b[39m\u001b[38;5;241m=\u001b[39mNUM_WORKERS\n\u001b[1;32m     17\u001b[0m     ):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Creates training and testing DataLoaders.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m        Takes in a training directory and testing directory path and turns\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m        them into PyTorch Datasets and then into PyTorch DataLoaders.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m                                    num_workers=4)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(test_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     43\u001b[0m     class_names \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mclasses\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/resizedImg/images/train'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64 # this is lower than the ViT paper but it's because we're starting small\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = create_dataloader(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, # use manually created transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf05aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:31.189209Z",
     "iopub.status.busy": "2023-11-01T09:33:31.188965Z",
     "iopub.status.idle": "2023-11-01T09:33:32.311478Z",
     "shell.execute_reply": "2023-11-01T09:33:32.310370Z",
     "shell.execute_reply.started": "2023-11-01T09:33:31.189188Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# Get a single image from the batch\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shapes\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50734d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:32.313233Z",
     "iopub.status.busy": "2023-11-01T09:33:32.312892Z",
     "iopub.status.idle": "2023-11-01T09:33:32.610858Z",
     "shell.execute_reply": "2023-11-01T09:33:32.609966Z",
     "shell.execute_reply.started": "2023-11-01T09:33:32.313204Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1, 2, 0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8bec02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:32.612414Z",
     "iopub.status.busy": "2023-11-01T09:33:32.612126Z",
     "iopub.status.idle": "2023-11-01T09:33:32.618898Z",
     "shell.execute_reply": "2023-11-01T09:33:32.617953Z",
     "shell.execute_reply.started": "2023-11-01T09:33:32.612389Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create example values\n",
    "height = 256 # H (\"The training resolution is 224.\")\n",
    "width = 256 # W\n",
    "color_channels = 3 # C\n",
    "patch_size = 64 # P\n",
    "\n",
    "# Calculate N (number of patches)\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71096d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:32.620823Z",
     "iopub.status.busy": "2023-11-01T09:33:32.620492Z",
     "iopub.status.idle": "2023-11-01T09:33:32.635722Z",
     "shell.execute_reply": "2023-11-01T09:33:32.634772Z",
     "shell.execute_reply.started": "2023-11-01T09:33:32.620796Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input shape (this is the size of a single image)\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "\n",
    "# Output shape\n",
    "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
    "\n",
    "print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bdc61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:32.637669Z",
     "iopub.status.busy": "2023-11-01T09:33:32.636964Z",
     "iopub.status.idle": "2023-11-01T09:33:32.879074Z",
     "shell.execute_reply": "2023-11-01T09:33:32.877942Z",
     "shell.execute_reply.started": "2023-11-01T09:33:32.637643Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View single image\n",
    "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e037b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:32.881106Z",
     "iopub.status.busy": "2023-11-01T09:33:32.880737Z",
     "iopub.status.idle": "2023-11-01T09:33:33.100075Z",
     "shell.execute_reply": "2023-11-01T09:33:33.098900Z",
     "shell.execute_reply.started": "2023-11-01T09:33:32.881073Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change image shape to be compatible with matplotlib (color_channels, height, width) -> (height, width, color_channels) \n",
    "image_permuted = image.permute(1, 2, 0)\n",
    "\n",
    "# Index to plot the top row of patched pixels\n",
    "patch_size = 4\n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, :, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a7a30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:33.102138Z",
     "iopub.status.busy": "2023-11-01T09:33:33.101691Z",
     "iopub.status.idle": "2023-11-01T09:33:33.325217Z",
     "shell.execute_reply": "2023-11-01T09:33:33.323920Z",
     "shell.execute_reply.started": "2023-11-01T09:33:33.102100Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
    "img_size = 256\n",
    "patch_size = 64\n",
    "num_patches = img_size/patch_size \n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \n",
    "print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# Create a series of subplots\n",
    "fig, axs = plt.subplots(nrows=1, \n",
    "                        ncols=img_size // patch_size, # one column for each patch\n",
    "                        figsize=(num_patches, num_patches),\n",
    "                        sharex=True,\n",
    "                        sharey=True)\n",
    "\n",
    "# Iterate through number of patches in the top row\n",
    "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
    "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n",
    "    axs[i].set_xlabel(i+1) # set the label\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fe283c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:33.328525Z",
     "iopub.status.busy": "2023-11-01T09:33:33.327559Z",
     "iopub.status.idle": "2023-11-01T09:33:33.334470Z",
     "shell.execute_reply": "2023-11-01T09:33:33.333119Z",
     "shell.execute_reply.started": "2023-11-01T09:33:33.328472Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup hyperparameters and make sure img_size and patch_size are compatible\n",
    "img_size = 256\n",
    "patch_size = 64\n",
    "num_patches = img_size/patch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56377463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:33.337481Z",
     "iopub.status.busy": "2023-11-01T09:33:33.336531Z",
     "iopub.status.idle": "2023-11-01T09:33:34.436946Z",
     "shell.execute_reply": "2023-11-01T09:33:34.435969Z",
     "shell.execute_reply.started": "2023-11-01T09:33:33.337433Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" \n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "# Create a series of subplots\n",
    "fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n",
    "                        ncols=img_size // patch_size, \n",
    "                        figsize=(num_patches, num_patches),\n",
    "                        sharex=True,\n",
    "                        sharey=True)\n",
    "\n",
    "# Loop through height and width of image\n",
    "for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n",
    "    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n",
    "        \n",
    "        # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels))\n",
    "        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height \n",
    "                                        patch_width:patch_width+patch_size, # iterate through width\n",
    "                                        :]) # get all color channels\n",
    "        \n",
    "        # Set up label information, remove the ticks for clarity and set labels to outside\n",
    "        axs[i, j].set_ylabel(i+1, \n",
    "                             rotation=\"horizontal\", \n",
    "                             horizontalalignment=\"right\", \n",
    "                             verticalalignment=\"center\") \n",
    "        axs[i, j].set_xlabel(j+1) \n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        axs[i, j].label_outer()\n",
    "\n",
    "# Set a super title\n",
    "fig.suptitle(f\"{class_names[label]} -> Patchified\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39760f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:34.439369Z",
     "iopub.status.busy": "2023-11-01T09:33:34.439013Z",
     "iopub.status.idle": "2023-11-01T09:33:34.520449Z",
     "shell.execute_reply": "2023-11-01T09:33:34.519454Z",
     "shell.execute_reply.started": "2023-11-01T09:33:34.439335Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Set the patch size\n",
    "patch_size=64\n",
    "\n",
    "# Create the Conv2d layer with hyperparameters from the ViT paper\n",
    "conv2d = nn.Conv2d(in_channels=3, # number of color channels\n",
    "                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n",
    "                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n",
    "                   stride=patch_size,\n",
    "                   padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e8dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:34.526613Z",
     "iopub.status.busy": "2023-11-01T09:33:34.526325Z",
     "iopub.status.idle": "2023-11-01T09:33:34.758969Z",
     "shell.execute_reply": "2023-11-01T09:33:34.757960Z",
     "shell.execute_reply.started": "2023-11-01T09:33:34.526588Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View single image\n",
    "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5205c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:34.760707Z",
     "iopub.status.busy": "2023-11-01T09:33:34.760338Z",
     "iopub.status.idle": "2023-11-01T09:33:34.840523Z",
     "shell.execute_reply": "2023-11-01T09:33:34.839391Z",
     "shell.execute_reply.started": "2023-11-01T09:33:34.760673Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass the image through the convolutional layer \n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -> (batch, height, width, color_channels)\n",
    "print(image_out_of_conv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb62570",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:34.842258Z",
     "iopub.status.busy": "2023-11-01T09:33:34.841866Z",
     "iopub.status.idle": "2023-11-01T09:33:34.870663Z",
     "shell.execute_reply": "2023-11-01T09:33:34.869744Z",
     "shell.execute_reply.started": "2023-11-01T09:33:34.842222Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_out_of_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9090d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:34.872472Z",
     "iopub.status.busy": "2023-11-01T09:33:34.871854Z",
     "iopub.status.idle": "2023-11-01T09:33:35.264573Z",
     "shell.execute_reply": "2023-11-01T09:33:35.263558Z",
     "shell.execute_reply.started": "2023-11-01T09:33:34.872439Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot random 5 convolutional feature maps\n",
    "import random\n",
    "random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\n",
    "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
    "\n",
    "# Create plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n",
    "\n",
    "# Plot random image feature maps\n",
    "for i, idx in enumerate(random_indexes):\n",
    "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
    "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
    "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ba5e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.267082Z",
     "iopub.status.busy": "2023-11-01T09:33:35.265992Z",
     "iopub.status.idle": "2023-11-01T09:33:35.275094Z",
     "shell.execute_reply": "2023-11-01T09:33:35.274084Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.267045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a single feature map in tensor form\n",
    "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
    "single_feature_map, single_feature_map.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf747d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.276725Z",
     "iopub.status.busy": "2023-11-01T09:33:35.276358Z",
     "iopub.status.idle": "2023-11-01T09:33:35.284515Z",
     "shell.execute_reply": "2023-11-01T09:33:35.283500Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.276663Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Current tensor shape\n",
    "print(f\"Current tensor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cdeed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.286076Z",
     "iopub.status.busy": "2023-11-01T09:33:35.285708Z",
     "iopub.status.idle": "2023-11-01T09:33:35.296294Z",
     "shell.execute_reply": "2023-11-01T09:33:35.294872Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.286045Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create flatten layer\n",
    "flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n",
    "                     end_dim=3) # flatten feature_map_width (dimension 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ee478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.298620Z",
     "iopub.status.busy": "2023-11-01T09:33:35.298086Z",
     "iopub.status.idle": "2023-11-01T09:33:35.614103Z",
     "shell.execute_reply": "2023-11-01T09:33:35.612966Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.298575Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. View single image\n",
    "plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# 2. Turn image into feature maps\n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\n",
    "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
    "\n",
    "# 3. Flatten the feature maps\n",
    "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
    "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf92d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.615759Z",
     "iopub.status.busy": "2023-11-01T09:33:35.615420Z",
     "iopub.status.idle": "2023-11-01T09:33:35.623930Z",
     "shell.execute_reply": "2023-11-01T09:33:35.622982Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.615729Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get flattened image patch embeddings in right shape \n",
    "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
    "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a27ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.625694Z",
     "iopub.status.busy": "2023-11-01T09:33:35.625347Z",
     "iopub.status.idle": "2023-11-01T09:33:35.787986Z",
     "shell.execute_reply": "2023-11-01T09:33:35.787056Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.625661Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get a single flattened feature map\n",
    "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n",
    "\n",
    "# Plot the flattened feature map visually\n",
    "plt.figure(figsize=(22, 22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e579f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.792787Z",
     "iopub.status.busy": "2023-11-01T09:33:35.792105Z",
     "iopub.status.idle": "2023-11-01T09:33:35.806811Z",
     "shell.execute_reply": "2023-11-01T09:33:35.805743Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.792745Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# See the flattened feature map as a tensor\n",
    "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca68002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.809858Z",
     "iopub.status.busy": "2023-11-01T09:33:35.809604Z",
     "iopub.status.idle": "2023-11-01T09:33:35.818217Z",
     "shell.execute_reply": "2023-11-01T09:33:35.817157Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.809836Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create a class which subclasses nn.Module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
    "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
    "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
    "    \"\"\" \n",
    "    # 2. Initialize the class with appropriate variables\n",
    "    def __init__(self, \n",
    "                 in_channels:int=1,\n",
    "                 patch_size:int=64,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create a layer to turn an image into patches\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
    "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
    "                                  end_dim=3)\n",
    "\n",
    "    # 5. Define the forward method \n",
    "    def forward(self, x):\n",
    "        # Create assertion to check that inputs are the correct shape\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "        \n",
    "        # Perform the forward pass\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "        # 6. Make sure the output shape has the right order \n",
    "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b849f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.820155Z",
     "iopub.status.busy": "2023-11-01T09:33:35.819823Z",
     "iopub.status.idle": "2023-11-01T09:33:35.941245Z",
     "shell.execute_reply": "2023-11-01T09:33:35.940095Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.820126Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# Create an instance of patch embedding layer\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                          patch_size=64,\n",
    "                          embedding_dim=768)\n",
    "\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bce6fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.943010Z",
     "iopub.status.busy": "2023-11-01T09:33:35.942636Z",
     "iopub.status.idle": "2023-11-01T09:33:35.948428Z",
     "shell.execute_reply": "2023-11-01T09:33:35.947359Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.942976Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create random input sizes\n",
    "random_input_image = (1, 3, 256, 256)\n",
    "random_input_image_error = (1, 3, 40, 40) # will error because image size is incompatible with patch_size\n",
    "\n",
    "# # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n",
    "# summary(PatchEmbedding(), \n",
    "#         input_size=random_input_image, # try swapping this for \"random_input_image_error\" \n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c9d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.949961Z",
     "iopub.status.busy": "2023-11-01T09:33:35.949627Z",
     "iopub.status.idle": "2023-11-01T09:33:35.962137Z",
     "shell.execute_reply": "2023-11-01T09:33:35.961167Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.949927Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the batch size and embedding dimension\n",
    "batch_size = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[-1]\n",
    "\n",
    "# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n",
    "                           requires_grad=True) # make sure the embedding is learnable\n",
    "\n",
    "# Show the first 10 examples of the class_token\n",
    "print(class_token[:, :, :10])\n",
    "\n",
    "# Print the class_token shape\n",
    "print(f\"Class token shape: {class_token.shape} -> [batch_size, number_of_tokens, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbdafa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.963755Z",
     "iopub.status.busy": "2023-11-01T09:33:35.963415Z",
     "iopub.status.idle": "2023-11-01T09:33:35.974188Z",
     "shell.execute_reply": "2023-11-01T09:33:35.973141Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.963729Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the class token embedding to the front of the patch embedding\n",
    "patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image), \n",
    "                                                      dim=1) # concat on first dimension\n",
    "\n",
    "# Print the sequence of patch embeddings with the prepended class token embedding\n",
    "print(patch_embedded_image_with_class_embedding)\n",
    "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce3882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.975694Z",
     "iopub.status.busy": "2023-11-01T09:33:35.975327Z",
     "iopub.status.idle": "2023-11-01T09:33:35.988845Z",
     "shell.execute_reply": "2023-11-01T09:33:35.987765Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.975671Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View the sequence of patch embeddings with the prepended class embedding\n",
    "patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dbde79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:35.990360Z",
     "iopub.status.busy": "2023-11-01T09:33:35.990045Z",
     "iopub.status.idle": "2023-11-01T09:33:36.004482Z",
     "shell.execute_reply": "2023-11-01T09:33:36.003332Z",
     "shell.execute_reply.started": "2023-11-01T09:33:35.990330Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate N (number of patches)\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "\n",
    "# Get embedding dimension\n",
    "embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n",
    "\n",
    "# Create the learnable 1D position embedding\n",
    "position_embedding = nn.Parameter(torch.ones(1,\n",
    "                                             number_of_patches+1, \n",
    "                                             embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\n",
    "print(position_embedding[:, :10, :10])\n",
    "print(f\"Position embeddding shape: {position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41f53b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.006091Z",
     "iopub.status.busy": "2023-11-01T09:33:36.005717Z",
     "iopub.status.idle": "2023-11-01T09:33:36.019051Z",
     "shell.execute_reply": "2023-11-01T09:33:36.018086Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.006060Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the position embedding to the patch and class token embedding\n",
    "patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\n",
    "print(patch_and_position_embedding)\n",
    "print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95290758",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.020657Z",
     "iopub.status.busy": "2023-11-01T09:33:36.020122Z",
     "iopub.status.idle": "2023-11-01T09:33:36.135549Z",
     "shell.execute_reply": "2023-11-01T09:33:36.134621Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.020626Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "# 1. Set patch size\n",
    "patch_size = 64\n",
    "\n",
    "# 2. Print shape of original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "\n",
    "# 5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True) # make sure it's learnable\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True) # make sure it's learnable\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b0d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.137189Z",
     "iopub.status.busy": "2023-11-01T09:33:36.136862Z",
     "iopub.status.idle": "2023-11-01T09:33:36.145050Z",
     "shell.execute_reply": "2023-11-01T09:33:36.143854Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.137164Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "        \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57626ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.147300Z",
     "iopub.status.busy": "2023-11-01T09:33:36.146615Z",
     "iopub.status.idle": "2023-11-01T09:33:36.232906Z",
     "shell.execute_reply": "2023-11-01T09:33:36.231793Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.147202Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of MSABlock\n",
    "multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1 \n",
    "                                                             num_heads=12) # from Table 1\n",
    "\n",
    "# Pass patch and position image embedding through MSABlock\n",
    "patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\n",
    "print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\n",
    "print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f144e02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.234906Z",
     "iopub.status.busy": "2023-11-01T09:33:36.234534Z",
     "iopub.status.idle": "2023-11-01T09:33:36.243295Z",
     "shell.execute_reply": "2023-11-01T09:33:36.242083Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.234873Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "    \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0931a9a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.245388Z",
     "iopub.status.busy": "2023-11-01T09:33:36.244967Z",
     "iopub.status.idle": "2023-11-01T09:33:36.318536Z",
     "shell.execute_reply": "2023-11-01T09:33:36.317436Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.245333Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of MLPBlock\n",
    "mlp_block = MLPBlock(embedding_dim=768, # from Table 1 \n",
    "                     mlp_size=3072, # from Table 1\n",
    "                     dropout=0.1) # from Table 3\n",
    "\n",
    "# Pass output of MSABlock through MLPBlock\n",
    "patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\n",
    "print(f\"Input shape of MLP block: {patched_image_through_mlp_block.shape}\")\n",
    "print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d702834e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.320476Z",
     "iopub.status.busy": "2023-11-01T09:33:36.320048Z",
     "iopub.status.idle": "2023-11-01T09:33:36.328524Z",
     "shell.execute_reply": "2023-11-01T09:33:36.327433Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.320442Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    "    # 5. Create a forward() method  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x =  self.msa_block(x) + x \n",
    "        \n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304fb3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:36.330515Z",
     "iopub.status.busy": "2023-11-01T09:33:36.329825Z",
     "iopub.status.idle": "2023-11-01T09:33:38.055769Z",
     "shell.execute_reply": "2023-11-01T09:33:38.054771Z",
     "shell.execute_reply.started": "2023-11-01T09:33:36.330482Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an instance of TransformerEncoderBlock\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 17, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ebad7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:38.057406Z",
     "iopub.status.busy": "2023-11-01T09:33:38.057119Z",
     "iopub.status.idle": "2023-11-01T09:33:38.119275Z",
     "shell.execute_reply": "2023-11-01T09:33:38.118398Z",
     "shell.execute_reply.started": "2023-11-01T09:33:38.057381Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the same as above with torch.nn.TransformerEncoderLayer()\n",
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                                                             nhead=12, # Heads from Table 1 for ViT-Base\n",
    "                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n",
    "                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                                                             activation=\"gelu\", # GELU non-linear activation\n",
    "                                                             batch_first=True, # Do our batches come first?\n",
    "                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n",
    "\n",
    "torch_transformer_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b487628",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:38.120577Z",
     "iopub.status.busy": "2023-11-01T09:33:38.120300Z",
     "iopub.status.idle": "2023-11-01T09:33:38.137410Z",
     "shell.execute_reply": "2023-11-01T09:33:38.136468Z",
     "shell.execute_reply.started": "2023-11-01T09:33:38.120553Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n",
    "summary(model=torch_transformer_encoder_layer,\n",
    "        input_size=(1, 17, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865cf73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:38.138772Z",
     "iopub.status.busy": "2023-11-01T09:33:38.138505Z",
     "iopub.status.idle": "2023-11-01T09:33:38.152063Z",
     "shell.execute_reply": "2023-11-01T09:33:38.151107Z",
     "shell.execute_reply.started": "2023-11-01T09:33:38.138743Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=256, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=64, # Patch size\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=4): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "        \n",
    "        # 3. Make the image size is divisble by the patch size \n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        \n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1) \n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b038c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:38.159282Z",
     "iopub.status.busy": "2023-11-01T09:33:38.158823Z",
     "iopub.status.idle": "2023-11-01T09:33:38.167422Z",
     "shell.execute_reply": "2023-11-01T09:33:38.166431Z",
     "shell.execute_reply.started": "2023-11-01T09:33:38.159243Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example of creating the class embedding and expanding over a batch dimension\n",
    "batch_size = 64\n",
    "class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\n",
    "class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n",
    "\n",
    "# Print out the change in shapes\n",
    "print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\") \n",
    "print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede9ae6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:38.168927Z",
     "iopub.status.busy": "2023-11-01T09:33:38.168555Z",
     "iopub.status.idle": "2023-11-01T09:33:38.181100Z",
     "shell.execute_reply": "2023-11-01T09:33:38.180266Z",
     "shell.execute_reply.started": "2023-11-01T09:33:38.168879Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc3e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:38.182286Z",
     "iopub.status.busy": "2023-11-01T09:33:38.182043Z",
     "iopub.status.idle": "2023-11-01T09:33:39.036208Z",
     "shell.execute_reply": "2023-11-01T09:33:39.035161Z",
     "shell.execute_reply.started": "2023-11-01T09:33:38.182265Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_seed()\n",
    "\n",
    "# # Create a random tensor with same shape as a single image\n",
    "random_image_tensor = torch.randn(1, 3, 256, 256) # (batch_size, color_channels, height, width)\n",
    "\n",
    "# # Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "vit = ViT(num_classes=len(class_names))\n",
    "\n",
    "# # Pass the random image tensor to our ViT instance\n",
    "vit(random_image_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc44f7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:39.037793Z",
     "iopub.status.busy": "2023-11-01T09:33:39.037466Z",
     "iopub.status.idle": "2023-11-01T09:33:41.903705Z",
     "shell.execute_reply": "2023-11-01T09:33:41.902797Z",
     "shell.execute_reply.started": "2023-11-01T09:33:39.037769Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
    "summary(model=vit, \n",
    "        input_size=(64, 3, 256, 256), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffe2b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:33:41.905359Z",
     "iopub.status.busy": "2023-11-01T09:33:41.905081Z",
     "iopub.status.idle": "2023-11-01T09:36:34.601742Z",
     "shell.execute_reply": "2023-11-01T09:36:34.600594Z",
     "shell.execute_reply.started": "2023-11-01T09:33:41.905337Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \n",
    "optimizer = torch.optim.Adam(params=vit.parameters(), \n",
    "                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n",
    "                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training & Fine-tuning)\n",
    "                             weight_decay=0.3) # from the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
    "\n",
    "# Setup the loss function for multi-class classification\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set the seeds\n",
    "set_seed()\n",
    "\n",
    "# Train the model and save the training results to a dictionary\n",
    "results = train(model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=15,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ff5a6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:36:34.603625Z",
     "iopub.status.busy": "2023-11-01T09:36:34.603280Z",
     "iopub.status.idle": "2023-11-01T09:36:35.421506Z",
     "shell.execute_reply": "2023-11-01T09:36:35.420480Z",
     "shell.execute_reply.started": "2023-11-01T09:36:34.603594Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(vit, 'ViT64-UnderBal_B64.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31ecdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-01T09:36:35.423172Z",
     "iopub.status.busy": "2023-11-01T09:36:35.422800Z",
     "iopub.status.idle": "2023-11-01T09:36:36.170708Z",
     "shell.execute_reply": "2023-11-01T09:36:36.169673Z",
     "shell.execute_reply.started": "2023-11-01T09:36:35.423139Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(vit.state_dict(), 'WeihtViT64-UnderBal_B64.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.990926,
   "end_time": "2023-11-01T09:38:21.599709",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T09:38:12.608783",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
